{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda812a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from interp.model.model_loading import load_model\n",
    "model_fn, params, tok = load_model(\n",
    "    \"jan5_attn_only_two_layers/\", \n",
    ")  # leave off models_dir for rrfs, using local is faster\n",
    "model = model_fn.bind(params)\n",
    "vocaby = {tok.decode([v]):v for v in range(len(tok))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc46fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ovs = np.array(model.get_ov_combined_mats_all_layers())\n",
    "qks = np.array(model.get_qk_combined_mats_all_layers())\n",
    "ovs_virtual = np.array(ovs[0].reshape(8,1,256,256) @ ovs[1].reshape(1,8,256,256))\n",
    "emb =np.array( params[\"params\"][\"embedding\"][\"token_embedding\"][\"embedding\"]).astype(np.float16)\n",
    "emb_m0 = emb-np.mean(emb,axis=0)\n",
    "emb_unit = emb_m0/np.linalg.norm(emb_m0,axis=1).reshape(-1,1)\n",
    "emb_small = emb[:10_000]\n",
    "emb_small_unit = emb_unit[:10_000]\n",
    "print(ovs.shape,qks.shape,ovs_virtual.shape,emb.shape,emb.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_to_bigrams(mat):\n",
    "    print(mat.shape)\n",
    "    bigmat = jnp.array(emb_small_unit) @ jnp.array(mat).astype(jnp.float16) @ jnp.array(emb_small_unit.T)\n",
    "    return np.array(bigmat.T)\n",
    "    \n",
    "def bigrams_top(bigrams,k=40):\n",
    "    bigrams = np.array(bigrams)\n",
    "    flat = bigrams.flatten()\n",
    "    sorted_idxs = np.argpartition(flat,-k)\n",
    "    tops = sorted_idxs[-k:]\n",
    "    tops_raveled = np.unravel_index(tops,bigrams.shape)\n",
    "    tops_tok_ids = [(tops_raveled[0][i],tops_raveled[1][i],flat[tops[i]]) for i in range(k)]\n",
    "    \n",
    "    sorted_idxs=None\n",
    "    sorted_idxs = np.argpartition(flat,k)\n",
    "    bottoms = sorted_idxs[:k]\n",
    "    bottoms_raveled = np.unravel_index(bottoms,bigrams.shape)\n",
    "    bottom_tok_ids = [(bottoms_raveled[0][i],bottoms_raveled[1][i],flat[bottoms[i]]) for i in range(k)]\n",
    "    return tops_tok_ids,bottom_tok_ids\n",
    "    \n",
    "def str_bigrams(bigrams):\n",
    "    return (\"\\n\".join([f\"'{tok.decode([f])}' -> '{tok.decode([t])}' : {v}\" for f,t,v in bigrams]))\n",
    "    \n",
    "def str_mat_bigrams(mat):\n",
    "    return \"\\n\\n\\n\".join([f\"{nm}\\n\\n{str_bigrams(x)}\" for nm,x in zip([\"top\",\"bottom\"],bigrams_top(mat_to_bigrams(mat)))])\n",
    "    \n",
    "print(str_mat_bigrams(ovs[1][5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661112ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ovs[1][5].shape)\n",
    "\n",
    "# I don't think this function is useful anymore\n",
    "def mat_to_copy_and_noncopy(mat):\n",
    "    eigenvals,eigenvecs = np.linalg.eig(mat)\n",
    "    real_mask = np.isreal(eigenvals) & np.all(np.isreal(eigenvecs),axis=0)\n",
    "    real_mat = np.real(eigenvecs@np.diag(eigenvals*real_mask)@eigenvecs.T)\n",
    "    imag_mat = np.real(eigenvecs@np.diag(eigenvals*(1-real_mask))@eigenvecs.T)\n",
    "    return real_mat,imag_mat\n",
    "    \n",
    "    \n",
    "def eig_by_angle(mat):\n",
    "    mn = 1e-4\n",
    "    # mn = np.mean(np.linalg.norm(mat,axis=1))*0.0001\n",
    "    eigenvals,eigenvecs = np.linalg.eig(mat)\n",
    "    inv_eigenvecs = np.linalg.inv(eigenvecs)\n",
    "    not_tiny_mask = np.absolute(eigenvals)>mn\n",
    "    eigenvals = eigenvals[not_tiny_mask]\n",
    "    eigenvecs = eigenvecs[:,not_tiny_mask]\n",
    "    inv_eigenvecs = inv_eigenvecs[not_tiny_mask]\n",
    "    angles =np.angle(eigenvals)\n",
    "    angles = np.amin(np.stack([np.abs(angles),np.abs(np.pi-angles)]),axis=0)\n",
    "    # sorted_idxs = np.argsort(angles)\n",
    "    sorted_idxs = np.argsort(-np.absolute(eigenvals))\n",
    "    return eigenvals[sorted_idxs],eigenvecs[:,sorted_idxs],inv_eigenvecs[sorted_idxs].T\n",
    "    \n",
    "    \n",
    "def svd_nont_tiny(mat):\n",
    "    u,s,vh = np.linalg.svd(mat)\n",
    "    not_tiny = s>1e-4\n",
    "    s=s[not_tiny]\n",
    "    u = u[:,not_tiny]\n",
    "    v = vh.T[:,not_tiny]\n",
    "    return s,u,v\n",
    "    \n",
    "def svd_inout(mat):\n",
    "    s,innie,outie = svd_nont_tiny(mat)\n",
    "    return innie*np.sqrt(s),outie*np.sqrt(s)\n",
    "\n",
    "def eig_inout(mat):\n",
    "    s,innie,outie = eig_by_angle(mat)\n",
    "    return innie*np.sqrt(s),outie*np.sqrt(s)\n",
    "    \n",
    "def format_top_logits(logit_shaped,k=20):\n",
    "    sorted_idxs = np.argsort(logit_shaped)\n",
    "    topk = sorted_idxs[-k:][::-1]\n",
    "    bottomk=sorted_idxs[:k]\n",
    "    topk_s = '\\n'.join([f\"{tok.decode([x])}\" for x in topk])\n",
    "    bottomk_s = '\\n'.join([f\"{tok.decode([x])}\" for x in bottomk])\n",
    "    string = f\"\\n{topk_s}\"\n",
    "    return string\n",
    "\n",
    "def show_in_outies(s,innies,outies):\n",
    "    emb_eigbasis_to = emb_small_unit@outies*np.sqrt(s)\n",
    "    emb_eigbasis_from = emb_small_unit@innies*np.sqrt(s)\n",
    "    strang = \"\"\n",
    "    for i in range(emb_eigbasis_to.shape[1]):\n",
    "        strang+=\"\\n\\nsingular val \"+str(s[i])\n",
    "        strang+=\"\\npos\"\n",
    "        strang+=f\"\\nfrom {jnp.amax(emb_eigbasis_from[:,i])}\"\n",
    "        strang+=format_top_logits(emb_eigbasis_from[:,i],5)\n",
    "        strang+=f\"\\nto {jnp.amax(emb_eigbasis_to[:,i])}\"\n",
    "        strang+=format_top_logits(emb_eigbasis_to[:,i],5)\n",
    "        strang+=\"\\nneg\"\n",
    "        strang+=f\"\\nfrom {jnp.amax(-emb_eigbasis_from[:,i])}\"\n",
    "        strang+=format_top_logits(-emb_eigbasis_from[:,i],5)\n",
    "        strang+=f\"\\nto {jnp.amax(-emb_eigbasis_to[:,i])}\"\n",
    "        strang+=format_top_logits(-emb_eigbasis_to[:,i],5)\n",
    "    return strang\n",
    "\n",
    "def next_type_copying_metric(ov,qk):\n",
    "    q_to_o = qk @ ov.T\n",
    "    \n",
    "    print(show_in_outies(*eig_by_angle(q_to_o)))\n",
    "    \n",
    "# print(show_in_outies(*eig_by_angle(ovs[1][5])))\n",
    "next_type_copying_metric(ovs[1,5],qks[1,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcb94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(2):\n",
    "    for head in range(8):\n",
    "        for qkovname,qkov in [(\"qk\",qks),(\"ov\",ovs)]:\n",
    "            fname = f\"/home/ubuntu/tokmaps_jan5/{qkovname}_{layer}.{head}.txt\"\n",
    "            strang=\"\"\n",
    "            strang+=show_in_outies(*eig_by_angle(qkov[layer,head]))\n",
    "            open(fname,\"w\").write(strang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bcc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to know what subspace is copied, what subspace is moved to what subspace, and what subspace is ignored. (4 subspaces).\n",
    "open_square_strs = [x for x in vocaby.keys() if (\"[\" in x) and len(set(\"[]()<>{}\").intersection(x))==1]\n",
    "close_square_strs = [x for x in vocaby.keys() if (\"]\" in x) and len(set(\"[]()<>{}\").intersection(x))==1]\n",
    "open_strs = [x for x in vocaby.keys() if (\"(\" in x) and len(set(\"[]()<>{}\").intersection(x))==1]\n",
    "close_strs = [x for x in vocaby.keys() if (\")\" in x) and len(set(\"[]()<>{}\").intersection(x))==1]\n",
    "print(open_square_strs,close_square_strs,open_strs,close_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_top_logits(((emb_unit[vocaby[\"}\"]]+emb_unit[vocaby[\"}\"]])@emb_unit.T).T,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import itertools\n",
    "import torch\n",
    "import os\n",
    "from interp.tools.data_loading import DATA_DIR\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def get_val_seqs():\n",
    "    fnames = os.listdir(f\"{DATA_DIR}/owt_tokens_int16_val\")[:2]\n",
    "    all_tokens = [torch.load(f\"{DATA_DIR}/owt_tokens_int16_val/{f}\") for f in fnames]\n",
    "    data_pt = list(itertools.chain(*[torch.split(x[\"tokens\"], x[\"lens\"].tolist()) for x in all_tokens]))\n",
    "\n",
    "    max_size = 511\n",
    "\n",
    "    data = torch.stack(\n",
    "        [data_pt_val[:max_size].to(torch.int64) + 32768 for data_pt_val in data_pt if data_pt_val.size(0) >= max_size],\n",
    "        dim=0,\n",
    "    ).numpy()\n",
    "    print(\"data shape\",data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d107b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_val_seqs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
