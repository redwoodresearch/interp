{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import os\n",
    "import flax\n",
    "\n",
    "name = \"gpt2-medium\"\n",
    "hf_model = transformers.AutoModelForCausalLM.from_pretrained(name)\n",
    "hf_params = hf_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_config = hf_model.config\n",
    "hf_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.model.gpt_model import Gpt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from interp.tools.interpretability_tools import batch_tokenize\n",
    "model = Gpt(num_layers=hf_config.n_layer,num_heads=hf_config.n_head,hidden_size = hf_config.n_embd,\n",
    "    max_sequence_len = hf_config.n_ctx,vocab_size=hf_config.vocab_size,use_mlp=True,norm_type=\"layer_norm\",attn_bias=True,layer_norm_epsilon=hf_config.layer_norm_epsilon)\n",
    "text = \"[BEGIN] \\\"I don't sleep right,\\\" Harry said.\"\n",
    "\n",
    "data = batch_tokenize([text])\n",
    "our_params = jax.jit(model.init)(jax.random.PRNGKey(0), data)[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def recurse(x):\n",
    "    if isinstance(x, jnp.ndarray):\n",
    "        return np.array(x)\n",
    "    return {k:recurse(v) for k,v in x.items()}\n",
    "out_params = recurse(our_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurse_print(x):\n",
    "    if isinstance(x, jnp.ndarray):\n",
    "        return x.shape\n",
    "    return {k:recurse_print(v) for k,v in x.items()}\n",
    "recurse_print(our_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually copying stuff over. Can check that all our keys are used, HF models have attention mask params and such taht we don't need. \n",
    "\n",
    "def cp(a,b):\n",
    "    assert tuple([x for x in a.shape])==tuple([x for x in b.shape]),(a.shape,b.shape)\n",
    "    np.copyto(b,np.array(a),casting='no')\n",
    "    \n",
    "cp(hf_params[\"transformer.wpe.weight\"], out_params[\"embedding\"][\"position_embedding\"][\"embedding\"])\n",
    "cp(hf_params[\"transformer.wte.weight\"], out_params[\"embedding\"][\"token_embedding\"][\"embedding\"])\n",
    "cp(hf_params[\"transformer.ln_f.bias\"], out_params[\"norm_output\"][\"bias\"])\n",
    "cp(hf_params[\"transformer.ln_f.weight\"], out_params[\"norm_output\"][\"scale\"])\n",
    "\n",
    "for i in range(hf_config.n_layer):\n",
    "    cp(hf_params[f\"transformer.h.{i}.ln_1.weight\"], out_params[f\"blocks_{i}\"][\"norm1\"][\"scale\"])\n",
    "    cp(hf_params[f\"transformer.h.{i}.ln_1.bias\"], out_params[f\"blocks_{i}\"][\"norm1\"][\"bias\"])\n",
    "    \n",
    "    cp(hf_params[f\"transformer.h.{i}.ln_2.weight\"], out_params[f\"blocks_{i}\"][\"norm2\"][\"scale\"])\n",
    "    cp(hf_params[f\"transformer.h.{i}.ln_2.bias\"], out_params[f\"blocks_{i}\"][\"norm2\"][\"bias\"])\n",
    "    \n",
    "    cp(hf_params[f\"transformer.h.{i}.attn.c_attn.weight\"], out_params[f\"blocks_{i}\"][\"attention\"][\"attn_weights\"][\"kernel\"])\n",
    "    cp(hf_params[f\"transformer.h.{i}.attn.c_attn.bias\"], out_params[f\"blocks_{i}\"][\"attention\"][\"attn_weights\"][\"bias\"])\n",
    "\n",
    "    cp(hf_params[f\"transformer.h.{i}.attn.c_proj.weight\"], out_params[f\"blocks_{i}\"][\"attention\"][\"project_output\"][\"kernel\"])\n",
    "    cp(hf_params[f\"transformer.h.{i}.attn.c_proj.bias\"], out_params[f\"blocks_{i}\"][\"attention\"][\"project_output\"][\"bias\"])\n",
    "    \n",
    "    cp(hf_params[f\"transformer.h.{i}.mlp.c_fc.bias\"], out_params[f\"blocks_{i}\"][\"linear1\"][\"bias\"])\n",
    "    cp(hf_params[f\"transformer.h.{i}.mlp.c_fc.weight\"], out_params[f\"blocks_{i}\"][\"linear1\"][\"kernel\"])\n",
    "    \n",
    "    cp(hf_params[f\"transformer.h.{i}.mlp.c_proj.bias\"], out_params[f\"blocks_{i}\"][\"linear2\"][\"bias\"])\n",
    "    cp(hf_params[f\"transformer.h.{i}.mlp.c_proj.weight\"], out_params[f\"blocks_{i}\"][\"linear2\"][\"kernel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_params_frozen = flax.core.frozen_dict.FrozenDict({\"params\":out_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_out = jax.nn.softmax(model.apply(out_params_frozen,data[:,1:])[0],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "hf_out = jax.nn.softmax(hf_model(t.tensor(np.array(data[:,1:]))).logits.detach().numpy(),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf_out[0,0,:12],our_out[0,0,:12])\n",
    "assert np.allclose(our_out,hf_out,atol=0.001) # not much precision, but i don't care that much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You have to create the model_info.json yourself because it's a sucky thing. Copy from another similar model\n",
    "from  flax.serialization import to_bytes\n",
    "import os\n",
    "local = True\n",
    "try:\n",
    "    os.mkdir(f\"/home/ubuntu{'' if local else '/rrfs'}/interpretability_models_jax/{name}\")\n",
    "except:\n",
    "    pass\n",
    "open(f\"/home/ubuntu{'' if local else '/rrfs'}/interpretability_models_jax/{name}/model.bin\",\"wb\").write(to_bytes(out_params_frozen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.model.model_loading import load_model\n",
    "model,params,tokenizer = load_model(name,models_dir=\"/home/ubuntu/interpretability_models_jax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2c98b1b4892158e45699de13196cffb209e9d071570faa659c9d1da64c1cf35"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
