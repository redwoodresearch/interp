{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from interp.model.model_loading import load_model, get_dataset_mean_activations\n",
    "from interp.tools.grad_modify_query import  ModifierCollectionTreeNode, Query, TargetConf, run_query\n",
    "from interp.tools.grad_modify_query_items import ItemConf, MulConf\n",
    "from interp.tools.grad_modify_query_utils import compose_trees\n",
    "from interp.tools.data_loading import get_val_seqs\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# model_name = \"GPT_canon\"\n",
    "model_name = \"GPT_canon\"\n",
    "# model_name = \"jan11_gpt_2l_canon\"\n",
    "# model_name = \"apr5_2l\"\n",
    "# model_name = \"jan5_attn_only_two_layers\"\n",
    "model, params, tokenizer = load_model(model_name, models_dir=\"/home/ubuntu/interpretability_models_jax\",\n",
    "load_params=True,)\n",
    "from interp.model.canon import canonicalize\n",
    "# params_canon = canonicalize(model,params, emb=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import interp.cui as cui\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from interp.ui.very_named_tensor import VeryNamedTensor\n",
    "# import interp.tensor_makers.atttention_weighted as aw\n",
    "# import interp.tensor_makers.correct_logprobs as cp\n",
    "# import interp.tensor_makers.logprobs as mp\n",
    "await cui.init(port=6789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from interp.model.canon import auto_canonicalize\n",
    "# params = auto_canonicalize(model,params)\n",
    "# from interp.model.model_loading import save_model\n",
    "# save_model(model,params,\"gpt2_canon\",\"GPT\",\"canonicalized gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import interp.ui.dataset_search as ds\n",
    "text = \"Yesterday John Von Neumann walked around the park with his dog. John Von Neumann was\"\n",
    "# text1 = \"On Saturday, Mr Smith went to the mall.\"\n",
    "# text2 = \"On Saturday, Mrs Smith went to the mall.\"\n",
    "token_ids = tokenizer(text, padding=False, return_tensors=\"jax\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.tools.log import LoggerCacheAll, MutLogCache,LogInfo\n",
    "\n",
    "def normalize(x):\n",
    "    return x/jnp.expand_dims(jnp.linalg.norm(x,axis=-1),-1)\n",
    "\n",
    "def get_mlp_stuff(layer):\n",
    "    i_kernel = jnp.transpose(params[\"params\"][f\"blocks_{layer}\"][\"linear1\"][\"kernel\"])\n",
    "    i_bias = params[\"params\"][f\"blocks_{layer}\"][\"linear1\"][\"bias\"]\n",
    "    o_kernel = params[\"params\"][f\"blocks_{layer}\"][\"linear2\"][\"kernel\"]\n",
    "    return i_kernel,i_bias,o_kernel\n",
    "mlps = [get_mlp_stuff(x) for x in range(model.num_layers)]\n",
    "ovs = model.bind(params).get_ov_combined_mats_all_layers()\n",
    "os = model.bind(params).get_o_mats_all_layers()\n",
    "token_embedding = params[\"params\"][\"embedding\"][\"token_embedding\"][\"embedding\"]\n",
    "pos_embedding = params[\"params\"][\"embedding\"][\"position_embedding\"][\"embedding\"]\n",
    "# tu,ts,tok_svd_transform = jnp.linalg.svd(token_embedding)\n",
    "# pu,ps,pos_svd_transform = jnp.linalg.svd(pos_embedding)\n",
    "mlps = [jnp.stack([mlp[k] for mlp in mlps],axis=0) for k in range(len(mlps[0]))]\n",
    "print([x.shape for x in mlps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o_argsort = jnp.argsort(jnp.linalg.norm(mlps[2],axis=-1),axis=-1)\n",
    "# i_by_mag = jnp.stack([mlps[0][l][o_argsort[l]] for l in range(model.num_layers)])\n",
    "# o_by_mag = jnp.stack([mlps[2][l][o_argsort[l]] for l in range(model.num_layers)])\n",
    "i_by_mag = mlps[0]\n",
    "o_by_mag = mlps[2]\n",
    "\n",
    "# print(i_by_mag.shape,o_by_mag.shape,jnp.argsort(jnp.linalg.norm(mlps[2],axis=-1),axis=-1).shape)\n",
    "dots = jnp.einsum(\"lni,lmi->lnm\",i_by_mag,o_by_mag)\n",
    "dots_i = jnp.einsum(\"lni,lmi->lnm\",i_by_mag,i_by_mag)\n",
    "dots_o = jnp.einsum(\"lni,lmi->lnm\",o_by_mag,o_by_mag)\n",
    "sims = jnp.einsum(\"lni,lmi->lnm\",normalize(i_by_mag),normalize(o_by_mag))\n",
    "sims_i = jnp.einsum(\"lni,lmi->lnm\",normalize(i_by_mag),normalize(i_by_mag))\n",
    "sims_o = jnp.einsum(\"lni,lmi->lnm\",normalize(o_by_mag),normalize(o_by_mag))\n",
    "tvnt = lambda x,t: vnt_guessing_shit_model_tokens(x,model,np.array(range(500)),title=t)\n",
    "from interp.ui.very_named_tensor import vnt_guessing_shit_model_tokens\n",
    "# await cui.show_tensors(tvnt(sims,\"io\"),tvnt(sims_i,\"ii\"),tvnt(sims_o,\"oo\"))\n",
    "iosim_argsort = jnp.argsort(sims[:,np.arange(model.hidden_size*4),np.arange(model.hidden_size*4)],axis=-1)\n",
    "print(iosim_argsort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tothing(x):\n",
    "    return jnp.mean(jnp.mean(x.at[:,np.arange(model.hidden_size*4),np.arange(model.hidden_size*4)].set(0),axis=-1),axis=-1)\n",
    "[]\n",
    "await cui.show_tensors(tvnt(sims[:,np.arange(model.hidden_size*4),np.arange(model.hidden_size*4)],\"sims\"),tvnt(mlps[1],\"bias\"),tvnt(dots[:,np.arange(model.hidden_size*4),np.arange(model.hidden_size*4)],\"dots\"),name=\"diag\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_direction_segment_through_mlp(start,end,layer):\n",
    "    vals = model.bind(params).blocks[layer].mlp(jnp.expand_dims(jnp.expand_dims(jnp.linspace(0,1,num=100,dtype=start.dtype),axis=1)*(end-start)+start,axis=0))\n",
    "    result = jnp.einsum(\"bsh,h->s\",vals,(end-start))\n",
    "    return result\n",
    "# px.line(plot_direction_segment_through_mlp(-mlps[0][0,1168],mlps[0][0,1166] ,0))\n",
    "\n",
    "# px.line(plot_direction_segment_through_mlp(-mlps[0][0,906],mlps[0][0,906] ,9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from interp.model.blocks import gelu\n",
    "def plot_mlp_input_graphs():\n",
    "    scale=1\n",
    "    vals = []\n",
    "    for layer in tqdm(range(model.num_layers)):\n",
    "        ivectors = mlps[0][layer]\n",
    "        out = model.bind(params).blocks[layer].mlp(jnp.expand_dims(jnp.linspace(0,1,num=50,dtype=ivectors.dtype),axis=1)*jnp.expand_dims(ivectors*2,axis=1)*scale+jnp.expand_dims(-ivectors*scale,axis=1))\n",
    "        vals.append( jnp.einsum(\"nsh,nh->ns\",out,ivectors*scale))\n",
    "    result = jnp.stack(vals)\n",
    "    return result\n",
    "def plot_mlp_input_graphs_nonorm():\n",
    "    scale=1\n",
    "    vals = []\n",
    "    for layer in tqdm(range(model.num_layers)):\n",
    "        ivectors = mlps[0][layer]\n",
    "        input = jnp.expand_dims(jnp.linspace(0,1,num=50,dtype=ivectors.dtype),axis=1)*jnp.expand_dims(ivectors*2,axis=1)*scale+jnp.expand_dims(-ivectors*scale,axis=1)\n",
    "        block = model.bind(params).blocks[layer]\n",
    "        out = block.linear2(gelu(block.linear1(input)))\n",
    "        vals.append( jnp.einsum(\"nsh,nh->ns\",out,ivectors*scale))\n",
    "    result = jnp.stack(vals)\n",
    "    return result\n",
    "def plot_mlp_input_graphs_single_neuron():\n",
    "    scale=1\n",
    "    vals = []\n",
    "    for layer in tqdm(range(model.num_layers)):\n",
    "        ivectors = mlps[0][layer]\n",
    "        input = jnp.expand_dims(jnp.linspace(0,1,num=50,dtype=ivectors.dtype),axis=1)*jnp.expand_dims(ivectors*2,axis=1)*scale+jnp.expand_dims(-ivectors*scale,axis=1) # neuron x h\n",
    "        block_params = model.bind(params).blocks[layer].variables[\"params\"]\n",
    "        neurons = gelu(jnp.einsum(\"hn,nxh->nx\",block_params[\"linear1\"][\"kernel\"],input)+jnp.expand_dims(block_params[\"linear1\"][\"bias\"],axis=1))\n",
    "        print(block_params[\"linear2\"][\"kernel\"].shape,neurons.shape,block_params[\"linear2\"][\"bias\"].shape)\n",
    "        out = jnp.einsum(\"nh,nx->nxh\",block_params[\"linear2\"][\"kernel\"],neurons)+block_params[\"linear2\"][\"bias\"]\n",
    "        out_in_dir = jnp.einsum(\"nxh,hn->nx\",out, block_params[\"linear1\"][\"kernel\"])*scale\n",
    "        vals.append( out_in_dir)\n",
    "    result = jnp.stack(vals)\n",
    "    return result\n",
    "single_neuron = plot_mlp_input_graphs_single_neuron()\n",
    "nonorm = plot_mlp_input_graphs_nonorm()\n",
    "withnorm = plot_mlp_input_graphs()\n",
    "stadked = jnp.stack([nonorm,single_neuron])\n",
    "\n",
    "\n",
    "stadked_byiosort = jnp.stack([stadked[:,l,iosim_argsort[l]] for l in range(model.num_layers)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stadked.shape,stadked_byiosort.shape)\n",
    "await cui.show_tensors(tvnt(stadked_byiosort[:,:,np.array([0,1,2,3])],\"top\"),tvnt(stadked_byiosort[:,:,np.array([1500,1501,1502,1503])],\"middle\"),tvnt(stadked_byiosort[:,:,np.array([-1,-2,-3,-4])],\"bottom\"),name=\"thang\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bind(params).blocks[0].linear1.variables[\"params\"][\"kernel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print([[tokenizer.decode([x]),i] for i,x in enumerate(token_ids[0])])\n",
    "embed_mask = np.zeros((token_ids.shape[1],),dtype=np.bool8)\n",
    "embed_mask[2] = 1\n",
    "head_mask = np.zeros((model.num_layers,model.num_heads, token_ids.shape[1]),dtype=np.bool8)\n",
    "head_mask[0,0,3] = 1\n",
    "head_mask[1,4,14] = 1\n",
    "mlp_mask = np.zeros((model.num_layers,model.hidden_size*4,token_ids.shape[1]),dtype=np.bool8)\n",
    "\n",
    "mlp_mask_tokenless = np.zeros((model.num_layers,model.hidden_size*4), dtype=np.bool8)\n",
    "head_mask_tokenless = np.zeros((model.num_layers,model.num_heads), dtype=np.bool8)\n",
    "head_mask_tokenless[0,7] = True\n",
    "head_mask_tokenless[1,7] = True\n",
    "# ds.embeds_through_mask_ablation(model,params,token_ids,embed_mask,head_mask,mlp_mask)\n",
    "ds.between_mask_ablation_unjit(model,params, token_ids,head_mask_tokenless,mlp_mask_tokenless,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.array(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.ui.attribution_backend import AttributionBackend\n",
    "backend = AttributionBackend(model, params, tokenizer, text)\n",
    "vnt_start = backend.startTree(\n",
    "    {\"kind\": \"logprob\", \"data\": {\"seqIdx\": 14, \"tokString\": \"Ne\", \"comparisonTokString\": None}}\n",
    ")\n",
    "_,_,masks = backend.searchAttributionsFromStart(0.05,True)\n",
    "print({k:jnp.sum(v) for k,v in masks.items()})\n",
    "masks_tokenless = {\"embeds\":np.any(masks[\"embeds\"],axis=-1,keepdims=True),\"heads\":np.any( masks[\"heads\"],axis=-1,keepdims=True),\"mlps\":np.any( masks[\"mlps\"],axis=-1,keepdims=True)}\n",
    "print(masks_tokenless)\n",
    "ds.embeds_through_mask_ablation(model,params,token_ids,masks[\"embeds\"],masks[\"heads\"],np.expand_dims(masks[\"mlps\"],axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import interp.cui as cui\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from interp.ui.very_named_tensor import VeryNamedTensor\n",
    "# import interp.tensor_makers.atttention_weighted as aw\n",
    "# import interp.tensor_makers.correct_logprobs as cp\n",
    "# import interp.tensor_makers.logprobs as mp\n",
    "await cui.init(port=6789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_tokenless[\"embeds\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ds.show_top_docs_fn(lambda token_ids: ds.embeds_through_mask_ablation(model,params,token_ids,masks_tokenless[\"embeds\"],masks_tokenless[\"heads\"],masks_tokenless[\"neurons\"]),n_seqs=100,batch_size=1,seq_len=64,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.tools.data_loading import get_val_seqs\n",
    "from interp.model.gpt_model import inference,inference_unjit\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_mean_activations(model,params, n=11000,batch_size=8,seq_len=512):\n",
    "    trimmed_n = (n//batch_size)*batch_size\n",
    "    seqs = get_val_seqs()[:trimmed_n,:seq_len]\n",
    "    seq_len = seqs.shape[-1]\n",
    "    \n",
    "    seqs = seqs.reshape(-1,batch_size,seq_len)\n",
    "    to_log = [f\"blocks.{l}.attention.out_by_head\" for l in range(model.num_layers)]\n",
    "    if model.use_mlp:\n",
    "        to_log.extend([f\"blocks.{l}.mlp.out_by_neuron\" for l in range(model.num_layers)])\n",
    "    to_log=tuple(to_log)\n",
    "    sums = None\n",
    "    for seq in tqdm(seqs):\n",
    "        _,log = inference(model,params,seq,to_log)\n",
    "        if sums==None:\n",
    "            sums = {k:jnp.mean(log[k],axis=0) for k in to_log}\n",
    "        else:\n",
    "            for k,v in sums.items():\n",
    "                sums[k] = v+jnp.mean(log[k],axis=0)\n",
    "    sums = {k:jnp.mean(v/seqs.shape[0],axis=1) for k,v in sums.items()}\n",
    "    stacked= {\"attention\":jnp.stack([sums[f\"blocks.{l}.attention.out_by_head\"] for l in range(model.num_layers)])}\n",
    "    if model.use_mlp:\n",
    "        stacked[\"mlp\"]=jnp.stack([sums[f\"blocks.{l}.mlp.out_by_neuron\"] for l in range(model.num_layers)])\n",
    "    return stacked\n",
    "mean_activations = collect_mean_activations(model,params,n=100,batch_size=6)\n",
    "print([k.shape for k in mean_activations.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flax.serialization import msgpack_serialize\n",
    "def save_all_avg_activations():\n",
    "    # for model_name in [\"jan5_attn_only_two_layers\", \"jan11_gpt_2l\", \"dec30_attn_only_two_layers\", \"GPT2\"]:\n",
    "    for model_name in [\"jan11_gpt_2l\"]:\n",
    "        model, params, tokenizer = load_model(model_name)\n",
    "        mean_activations = collect_mean_activations(model,params,batch_size=1,seq_len=64)\n",
    "        try:\n",
    "            os.mkdir(f\"/home/ubuntu/interpretability_model_stats/{model_name}\")\n",
    "        except:\n",
    "            pass\n",
    "        open(f\"/home/ubuntu/interpretability_model_stats/{model_name}/mean_activations\",\"wb\").write(msgpack_serialize(mean_activations))\n",
    "save_all_avg_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log = LogCacheAll()\n",
    "model_b(val_seqs[0:1], log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log['blocks.1.mlp.linear1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "A = ds.get_mlp_stats_unjit(model, params, input_ids=val_seqs[1:5])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_search import *\n",
    "\n",
    "def get_mlp_grads_unjit(model, params, input_ids_batch,l):\n",
    "\n",
    "    result = run_query(\n",
    "        loss_by_token_loggable(model, params, input_ids),\n",
    "        query=Query(\n",
    "            targets=[TargetConf(\"loss.lm_loss\", display_name=\"loss\")],\n",
    "            modifier_collection_tree=ModifierCollectionTreeNode(MulConf(ItemConf(f\"blocks.{l}.mlp.linear1\"),shape=I[1,1,4*model.hidden_size])),\n",
    "            use_fwd=False,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    result = result['loss'][:,:,0,0,:]\n",
    "    return result\n",
    "\n",
    "dataset_here = get_val_seqs()\n",
    "batch = dataset_here[:2,:128]\n",
    "# dataset_batches = dataset_here.reshape(-1, 10, 100)\n",
    "# print(dataset_batches.shape)\n",
    "result = get_mlp_grads_unjit(model, params, batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from interp.ui.dataset_search import get_mlp_stats\n",
    "\n",
    "n_seqs = 3000\n",
    "batch_size = 10\n",
    "seq_len = 100\n",
    "k = 5\n",
    "agg = 'mean'\n",
    "\n",
    "assert agg in {'mean', 'max'}\n",
    "dataset_here = get_val_seqs()[: (n_seqs // batch_size) * batch_size, :seq_len]\n",
    "dataset_batches = dataset_here.reshape(-1, batch_size, seq_len)\n",
    "result = []\n",
    "top_k_doc_idx = np.zeros((k, model.num_layers, model.hidden_size*4), dtype=np.int32)\n",
    "top_k_vals = -np.infty*np.ones((k, model.num_layers, model.hidden_size*4))\n",
    "for i, batch in tqdm(enumerate(dataset_batches)):\n",
    "    activations = np.array(get_mlp_stats(model, params, batch), dtype=np.float16)\n",
    "    if agg == 'mean':\n",
    "        per_batch_vals = activations.mean(axis=-2)\n",
    "    elif agg == 'max':\n",
    "        per_batch_vals = activations.max(axis=-2)\n",
    "    else:\n",
    "        assert False,\"agg should be in {'mean', 'max'} (and an assertion should have caught it earlier)\"\n",
    "\n",
    "    \n",
    "    batch_and_top_k_vals = np.concatenate((per_batch_vals, top_k_vals), axis=0)\n",
    "    bk_top_idx = batch_and_top_k_vals.argsort(axis=0)[-k:]\n",
    "    top_k_vals = np.take_along_axis(batch_and_top_k_vals, bk_top_idx, axis=0)\n",
    "    \n",
    "    new_k_idx, l_idx, n_idx = np.where(bk_top_idx >= batch_size)\n",
    "    top_k_doc_idx[new_k_idx, l_idx, n_idx] = top_k_doc_idx[bk_top_idx[new_k_idx, l_idx, n_idx] - batch_size, l_idx, n_idx]\n",
    "    top_k_doc_idx[bk_top_idx < batch_size] = bk_top_idx[bk_top_idx < batch_size] + i*batch_size\n",
    "\n",
    "top_k_doc_idx, top_k_vals = np.moveaxis(top_k_doc_idx, 0, -1), np.moveaxis(top_k_vals, 0, -1)    \n",
    "top_k_doc_idx, top_k_vals = np.flip(top_k_doc_idx, axis=-1), np.flip(top_k_vals, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(tokenizer.decode(dataset_here[top_k_doc_idx[1,8]][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.ui.dataset_search import *\n",
    "\n",
    "\n",
    "def get_mlp_grads_unjit(model, params, input_ids, l):\n",
    "\n",
    "    result = run_query(\n",
    "        loss_by_token_loggable(model, params, input_ids),\n",
    "        query=Query(\n",
    "            targets=[TargetConf(\"loss.lm_loss\", display_name=\"loss\")],\n",
    "            modifier_collection_tree=ModifierCollectionTreeNode(MulConf(ItemConf(f\"blocks.{l}.mlp.linear1\"),shape=[1,1,model.hidden_size * 4])),\n",
    "            use_fwd=True,\n",
    "        ),\n",
    "    )\n",
    "    print(\"RESULT SHAPE\", result.shape)\n",
    "    return result\n",
    "\n",
    "get_mlp_grads_unjit(model, params, input_ids=dataset_batches[1:2], l=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['loss'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['loss'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k_doc_idx[new_top_k_idx < batch_size] = new_top_k_idx[new_top_k_idx < batch_size] + i*batch_size\n",
    "# top_k_doc_idx[new_top_k_idx >= batch_size] = \n",
    "k_and_doc_idx, layer_idx, neuron_idx = np.where(new_top_k_idx >= batch_size)\n",
    "k_and_doc_idx-batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A[A.argsort(axis=0)[-2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log.cache['blocks.1.mlp.gelu'].shape)\n",
    "print(log.cache['blocks.1.mlp.gelu.identity'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = set(log.cache.keys())\n",
    "{k for k in keys if ('mlp' in k) and ('blocks.0' in k) and ('norm' not in k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ds.show_path_contribution_histogram(model,params,((1,5),),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2c98b1b4892158e45699de13196cffb209e9d071570faa659c9d1da64c1cf35"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
