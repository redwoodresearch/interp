{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't find models at /home/ubuntu/rrfs/interpretability_models_jax//jan5_attn_only_two_layers Cache them locally with `rsync -r ~/rrfs/interpretability_models_jax/ /home/ubuntu/rrfs/interpretability_models_jax//jan5_attn_only_two_layers\n",
      "loading jan5_attn_only_two_layers from /home/ubuntu/rrfs/interpretability_models_jax//jan5_attn_only_two_layers\n",
      "Can't find models at /home/ubuntu/rrfs/interpretability_models_jax//GPT2 Cache them locally with `rsync -r ~/rrfs/interpretability_models_jax/ /home/ubuntu/rrfs/interpretability_models_jax//GPT2\n",
      "loading GPT2 from /home/ubuntu/rrfs/interpretability_models_jax//GPT2\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from interp.model.model_loading import load_model, get_dataset_mean_activations\n",
    "from interp.tools.grad_modify_query import  ModifierCollectionTreeNode, Query, TargetConf, run_query\n",
    "from interp.tools.grad_modify_query_items import ItemConf, MulConf\n",
    "from interp.tools.grad_modify_query_utils import compose_trees\n",
    "from interp.tools.data_loading import get_val_seqs\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "# model2_name = \"GPT2\"\n",
    "model2_name = \"apr5_2l\"\n",
    "model_name = \"jan5_attn_only_two_layers\"\n",
    "model, params, tokenizer = load_model(model_name, models_dir=\"/home/ubuntu/interpretability_models_jax\")\n",
    "model_ao, params_ao, tokenizer = load_model(model2_name, models_dir=\"/home/ubuntu/interpretability_models_jax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import interp.ui.dataset_search as ds\n",
    "text = \"Yesterday John Von Neumann walked around the park with his dog. John Von Neumann was\"\n",
    "# text1 = \"On Saturday, Mr Smith went to the mall.\"\n",
    "# text2 = \"On Saturday, Mrs Smith went to the mall.\"\n",
    "token_ids = tokenizer(text, padding=False, return_tensors=\"jax\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:03<00:00, 80.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DeviceArray(2.385, dtype=float16), DeviceArray(-1.0932499, dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result,_=ds.run_fn_on_dataset(partial(ds.diff_between_model_losses,model,params,model_ao,params_ao),n_seqs=1000)\n",
    "[jnp.std(result),jnp.mean(result.astype(jnp.float64))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ds.show_top_docs_fn(partial(ds.diff_between_model_losses,model,params,model_ao,params_ao),n_seqs=20,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.model.gpt_model import inference\n",
    "from interp.tools.log import KeyIdxs, Idxs, LoggerCache,MutLogCache,SubLogCache,LoggerCacheAll\n",
    "\n",
    "logger = LoggerCacheAll()\n",
    "# logger.add(KeyIdxs(\"blocks.mlp.out_by_neuron\", Idxs.all()))\n",
    "log =  MutLogCache.new(logger=logger)\n",
    "model.bind(params).blocks[0].mlp(np.random.normal(0,1,(1,10,256)),log=log.sub(\"blocks\").sub(\"mlp\"))\n",
    "print(log.cache.idxed_cache.keys(),log.cache.cache.keys(),log.cache.cache[\"blocks.mlp.out_by_neuron\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print([[tokenizer.decode([x]),i] for i,x in enumerate(token_ids[0])])\n",
    "embed_mask = np.zeros((token_ids.shape[1],),dtype=np.bool8)\n",
    "embed_mask[2] = 1\n",
    "head_mask = np.zeros((model.num_layers,model.num_heads, token_ids.shape[1]),dtype=np.bool8)\n",
    "head_mask[0,0,3] = 1\n",
    "head_mask[1,4,14] = 1\n",
    "mlp_mask = np.zeros((model.num_layers,model.hidden_size*4,token_ids.shape[1]),dtype=np.bool8)\n",
    "\n",
    "mlp_mask_tokenless = np.zeros((model.num_layers,model.hidden_size*4), dtype=np.bool8)\n",
    "head_mask_tokenless = np.zeros((model.num_layers,model.num_heads), dtype=np.bool8)\n",
    "head_mask_tokenless[0,7] = True\n",
    "head_mask_tokenless[1,7] = True\n",
    "# ds.embeds_through_mask_ablation(model,params,token_ids,embed_mask,head_mask,mlp_mask)\n",
    "ds.between_mask_ablation_unjit(model,params, token_ids,head_mask_tokenless,mlp_mask_tokenless,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset_mean_activations(model_name).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.ui.attribution_backend import AttributionBackend\n",
    "backend = AttributionBackend(model, params, tokenizer, text)\n",
    "vnt_start = backend.startTree(\n",
    "    {\"kind\": \"logprob\", \"data\": {\"seqIdx\": 14, \"tokString\": \"Ne\", \"comparisonTokString\": None}}\n",
    ")\n",
    "_,_,masks = backend.searchAttributionsFromStart(0.05,True)\n",
    "print({k:jnp.sum(v) for k,v in masks.items()})\n",
    "masks_tokenless = {\"embeds\":np.any(masks[\"embeds\"],axis=-1,keepdims=True),\"heads\":np.any( masks[\"heads\"],axis=-1,keepdims=True),\"mlps\":np.any( masks[\"mlps\"],axis=-1,keepdims=True)}\n",
    "print(masks_tokenless)\n",
    "ds.embeds_through_mask_ablation(model,params,token_ids,masks[\"embeds\"],masks[\"heads\"],np.expand_dims(masks[\"mlps\"],axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import interp.cui as cui\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from interp.ui.very_named_tensor import VeryNamedTensor\n",
    "# import interp.tensor_makers.atttention_weighted as aw\n",
    "# import interp.tensor_makers.correct_logprobs as cp\n",
    "# import interp.tensor_makers.logprobs as mp\n",
    "await cui.init(port=6789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_tokenless[\"embeds\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ds.show_top_docs_fn(lambda token_ids: ds.embeds_through_mask_ablation(model,params,token_ids,masks_tokenless[\"embeds\"],masks_tokenless[\"heads\"],masks_tokenless[\"neurons\"]),n_seqs=100,batch_size=1,seq_len=64,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.tools.data_loading import get_val_seqs\n",
    "from interp.model.gpt_model import inference,inference_unjit\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_mean_activations(model,params, n=11000,batch_size=8,seq_len=512):\n",
    "    trimmed_n = (n//batch_size)*batch_size\n",
    "    seqs = get_val_seqs()[:trimmed_n,:seq_len]\n",
    "    seq_len = seqs.shape[-1]\n",
    "    \n",
    "    seqs = seqs.reshape(-1,batch_size,seq_len)\n",
    "    to_log = [f\"blocks.{l}.attention.out_by_head\" for l in range(model.num_layers)]\n",
    "    if model.use_mlp:\n",
    "        to_log.extend([f\"blocks.{l}.mlp.out_by_neuron\" for l in range(model.num_layers)])\n",
    "    to_log=tuple(to_log)\n",
    "    sums = None\n",
    "    for seq in tqdm(seqs):\n",
    "        _,log = inference(model,params,seq,to_log)\n",
    "        if sums==None:\n",
    "            sums = {k:jnp.mean(log[k],axis=0) for k in to_log}\n",
    "        else:\n",
    "            for k,v in sums.items():\n",
    "                sums[k] = v+jnp.mean(log[k],axis=0)\n",
    "    sums = {k:jnp.mean(v/seqs.shape[0],axis=1) for k,v in sums.items()}\n",
    "    stacked= {\"attention\":jnp.stack([sums[f\"blocks.{l}.attention.out_by_head\"] for l in range(model.num_layers)])}\n",
    "    if model.use_mlp:\n",
    "        stacked[\"mlp\"]=jnp.stack([sums[f\"blocks.{l}.mlp.out_by_neuron\"] for l in range(model.num_layers)])\n",
    "    return stacked\n",
    "mean_activations = collect_mean_activations(model,params,n=100,batch_size=6)\n",
    "print([k.shape for k in mean_activations.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flax.serialization import msgpack_serialize\n",
    "def save_all_avg_activations():\n",
    "    # for model_name in [\"jan5_attn_only_two_layers\", \"jan11_gpt_2l\", \"dec30_attn_only_two_layers\", \"GPT2\"]:\n",
    "    for model_name in [\"jan11_gpt_2l\"]:\n",
    "        model, params, tokenizer = load_model(model_name)\n",
    "        mean_activations = collect_mean_activations(model,params,batch_size=1,seq_len=64)\n",
    "        try:\n",
    "            os.mkdir(f\"/home/ubuntu/interpretability_model_stats/{model_name}\")\n",
    "        except:\n",
    "            pass\n",
    "        open(f\"/home/ubuntu/interpretability_model_stats/{model_name}/mean_activations\",\"wb\").write(msgpack_serialize(mean_activations))\n",
    "save_all_avg_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log = LogCacheAll()\n",
    "model_b(val_seqs[0:1], log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log['blocks.1.mlp.linear1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "A = ds.get_mlp_stats_unjit(model, params, input_ids=val_seqs[1:5])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_search import *\n",
    "\n",
    "def get_mlp_grads_unjit(model, params, input_ids_batch,l):\n",
    "\n",
    "    result = run_query(\n",
    "        loss_by_token_loggable(model, params, input_ids),\n",
    "        query=Query(\n",
    "            targets=[TargetConf(\"loss.lm_loss\", display_name=\"loss\")],\n",
    "            modifier_collection_tree=ModifierCollectionTreeNode(MulConf(ItemConf(f\"blocks.{l}.mlp.linear1\"),shape=I[1,1,4*model.hidden_size])),\n",
    "            use_fwd=False,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    result = result['loss'][:,:,0,0,:]\n",
    "    return result\n",
    "\n",
    "dataset_here = get_val_seqs()\n",
    "batch = dataset_here[:2,:128]\n",
    "# dataset_batches = dataset_here.reshape(-1, 10, 100)\n",
    "# print(dataset_batches.shape)\n",
    "result = get_mlp_grads_unjit(model, params, batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from interp.ui.dataset_search import get_mlp_stats\n",
    "\n",
    "n_seqs = 3000\n",
    "batch_size = 10\n",
    "seq_len = 100\n",
    "k = 5\n",
    "agg = 'mean'\n",
    "\n",
    "assert agg in {'mean', 'max'}\n",
    "dataset_here = get_val_seqs()[: (n_seqs // batch_size) * batch_size, :seq_len]\n",
    "dataset_batches = dataset_here.reshape(-1, batch_size, seq_len)\n",
    "result = []\n",
    "top_k_doc_idx = np.zeros((k, model.num_layers, model.hidden_size*4), dtype=np.int32)\n",
    "top_k_vals = -np.infty*np.ones((k, model.num_layers, model.hidden_size*4))\n",
    "for i, batch in tqdm(enumerate(dataset_batches)):\n",
    "    activations = np.array(get_mlp_stats(model, params, batch), dtype=np.float16)\n",
    "    if agg == 'mean':\n",
    "        per_batch_vals = activations.mean(axis=-2)\n",
    "    elif agg == 'max':\n",
    "        per_batch_vals = activations.max(axis=-2)\n",
    "    else:\n",
    "        assert False,\"agg should be in {'mean', 'max'} (and an assertion should have caught it earlier)\"\n",
    "\n",
    "    \n",
    "    batch_and_top_k_vals = np.concatenate((per_batch_vals, top_k_vals), axis=0)\n",
    "    bk_top_idx = batch_and_top_k_vals.argsort(axis=0)[-k:]\n",
    "    top_k_vals = np.take_along_axis(batch_and_top_k_vals, bk_top_idx, axis=0)\n",
    "    \n",
    "    new_k_idx, l_idx, n_idx = np.where(bk_top_idx >= batch_size)\n",
    "    top_k_doc_idx[new_k_idx, l_idx, n_idx] = top_k_doc_idx[bk_top_idx[new_k_idx, l_idx, n_idx] - batch_size, l_idx, n_idx]\n",
    "    top_k_doc_idx[bk_top_idx < batch_size] = bk_top_idx[bk_top_idx < batch_size] + i*batch_size\n",
    "\n",
    "top_k_doc_idx, top_k_vals = np.moveaxis(top_k_doc_idx, 0, -1), np.moveaxis(top_k_vals, 0, -1)    \n",
    "top_k_doc_idx, top_k_vals = np.flip(top_k_doc_idx, axis=-1), np.flip(top_k_vals, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(tokenizer.decode(dataset_here[top_k_doc_idx[1,8]][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.ui.dataset_search import *\n",
    "\n",
    "\n",
    "def get_mlp_grads_unjit(model, params, input_ids, l):\n",
    "\n",
    "    result = run_query(\n",
    "        loss_by_token_loggable(model, params, input_ids),\n",
    "        query=Query(\n",
    "            targets=[TargetConf(\"loss.lm_loss\", display_name=\"loss\")],\n",
    "            modifier_collection_tree=ModifierCollectionTreeNode(MulConf(ItemConf(f\"blocks.{l}.mlp.linear1\"),shape=[1,1,model.hidden_size * 4])),\n",
    "            use_fwd=True,\n",
    "        ),\n",
    "    )\n",
    "    print(\"RESULT SHAPE\", result.shape)\n",
    "    return result\n",
    "\n",
    "get_mlp_grads_unjit(model, params, input_ids=dataset_batches[1:2], l=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['loss'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['loss'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k_doc_idx[new_top_k_idx < batch_size] = new_top_k_idx[new_top_k_idx < batch_size] + i*batch_size\n",
    "# top_k_doc_idx[new_top_k_idx >= batch_size] = \n",
    "k_and_doc_idx, layer_idx, neuron_idx = np.where(new_top_k_idx >= batch_size)\n",
    "k_and_doc_idx-batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A[A.argsort(axis=0)[-2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log.cache['blocks.1.mlp.gelu'].shape)\n",
    "print(log.cache['blocks.1.mlp.gelu.identity'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = set(log.cache.keys())\n",
    "{k for k in keys if ('mlp' in k) and ('blocks.0' in k) and ('norm' not in k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ds.show_path_contribution_histogram(model,params,((1,5),),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2c98b1b4892158e45699de13196cffb209e9d071570faa659c9d1da64c1cf35"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
